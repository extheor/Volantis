<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>码小余の文档</title>
  
  
  <link href="/volantis/atom.xml" rel="self"/>
  
  <link href="http://extheor.club/volantis/"/>
  <updated>2021-03-20T16:00:00.000Z</updated>
  <id>http://extheor.club/volantis/</id>
  
  <author>
    <name>码小余</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Regression - Case Study</title>
    <link href="http://extheor.club/volantis/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Regressio%20-%20Case%20Study/"/>
    <id>http://extheor.club/volantis/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Regressio%20-%20Case%20Study/</id>
    <published>2021-03-20T16:00:00.000Z</published>
    <updated>2021-03-20T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Regression-Case-Study"><a href="#Regression-Case-Study" class="headerlink" title="Regression - Case Study"></a>Regression - Case Study</h1><blockquote><p><strong>回归</strong>-案例研究</p></blockquote><h4 id="问题的导入：预测宝可梦的CP值"><a href="#问题的导入：预测宝可梦的CP值" class="headerlink" title="问题的导入：预测宝可梦的CP值"></a>问题的导入：预测宝可梦的CP值</h4><p>Estimating the Combat Power(CP) of a pokemon after evolution </p><p>我们期望根据已有的宝可梦进化前后的信息，来预测某只宝可梦进化后的cp值的大小</p><h4 id="确定Senario、Task和Model"><a href="#确定Senario、Task和Model" class="headerlink" title="确定Senario、Task和Model"></a>确定Senario、Task和Model</h4><h5 id="Senario"><a href="#Senario" class="headerlink" title="Senario"></a>Senario</h5><p>首先根据已有的data来确定Senario，我们拥有宝可梦进化前后cp值的这样一笔数据，input是进化前的宝可梦(包括它的各种属性)，output是进化后的宝可梦的cp值；因此我们的data是labeled，使用的Senario是<strong>Supervised Learning</strong></p><h5 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h5><p>然后根据我们想要function的输出类型来确定Task，我们预期得到的是宝可梦进化后的cp值，是一个scalar，因此使用的Task是<strong>Regression</strong></p><h5 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h5><p>关于Model，选择很多，这里采用的是<strong>Non-linear Model</strong></p><h4 id="设定具体参数"><a href="#设定具体参数" class="headerlink" title="设定具体参数"></a>设定具体参数</h4><p>$X$：   表示一只宝可梦，用下标表示该宝可梦的某种属性</p><p>$X_{cp}$：表示该宝可梦进化前的cp值</p><p>$X_{s}$：  表示该宝可梦是属于哪一种物种，比如妙瓜种子、皮卡丘…</p><p>$X_{hp}$：表示该宝可梦的hp值即生命值是多少</p><p>$X_{w}$： 代表该宝可梦的重重量</p><p>$X_{h}$： 代表该宝可梦的高度</p><p>$f()$： 表示我们要找的function</p><p>$y$：    表示function的output，即宝可梦进化后的cp值，是一个scalar</p><p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/pokeman-parameters.png" class="lazyload" data-srcset="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/pokeman-parameters.png" srcset="data:image/png;base64,666" alt="pokeman-parameters"></p><h4 id="Regression的具体过程"><a href="#Regression的具体过程" class="headerlink" title="Regression的具体过程"></a>Regression的具体过程</h4><h5 id="回顾一下machine-Learning的三个步骤："><a href="#回顾一下machine-Learning的三个步骤：" class="headerlink" title="回顾一下machine Learning的三个步骤："></a>回顾一下machine Learning的三个步骤：</h5><ul><li>定义一个model即function set</li><li>定义一个goodness of function损失函数去评估该function的好坏</li><li>找一个最好的function</li></ul><h5 id="Step1：Model-function-set"><a href="#Step1：Model-function-set" class="headerlink" title="Step1：Model (function set)"></a>Step1：Model (function set)</h5><p>如何选择一个function的模型呢？毕竟只有确定了模型才能调参。这里没有明确的思路，只能凭经验去一种种地试</p><h6 id="Linear-Model-线性模型"><a href="#Linear-Model-线性模型" class="headerlink" title="Linear Model 线性模型"></a>Linear Model 线性模型</h6><p>$y=b+w* X_{cp}$</p><p>y代表进化后的cp值，代表进化前的cp值，w和b代表未知参数，可以是任何数值</p><p>根据不同的w和b，可以确定不同的无穷无尽的function，而这个抽象出来的式子就叫做model，是以上这些具体化的function的集合，即function set</p><p>实际上这是一种<strong>Linear Model</strong>，但只考虑了宝可梦进化前的cp值，因而我们可以将其扩展为：</p><font style="background: yellow;">$y=b+w* X_{cp}$</font><p><strong>xi</strong>： an attribute of input X  ( xi is also called <strong>feature</strong>，即特征值)</p><p><strong>wi</strong>：weight of xi</p><p><strong>b</strong>：  bias</p><p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/model.png" class="lazyload" data-srcset="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/model.png" srcset="data:image/png;base64,666" alt="model"></p><h5 id="Step2：Goodness-of-Function"><a href="#Step2：Goodness-of-Function" class="headerlink" title="Step2：Goodness of Function"></a>Step2：Goodness of Function</h5><h6 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h6><p>$x^i$：用上标来表示一个完整的object的编号，$x^i$表示第i只宝可梦(下标表示该object中的component)</p><p>$\hat{y}^i$：用表示一个实际观察到的object输出，上标为i表示是第i个object</p><p>注：由于regression的输出值是scalar，因此$\hat{y}^i$里面并没有component，只是一个简单的数值；但是未来如果考虑structured Learning的时候，我们output的object可能是有structured的，所以我们还是会需要用上标下标来表示一个完整的output的object和它包含的component</p><p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/goodness-of-function.png" class="lazyload" data-srcset="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/goodness-of-function.png" srcset="data:image/png;base64,666" alt="goodness-of-function"></p><h6 id="Loss-function-损失函数"><a href="#Loss-function-损失函数" class="headerlink" title="Loss function 损失函数"></a>Loss function 损失函数</h6><p>为了衡量function set中的某个function的好坏，我们需要一个评估函数，即<font style="background: yellow;">Loss function</font>，损失函数，简称<code>L</code>；<code>loss function</code>是一个function的function</p><p>$L(f)=L(w,b)$</p><p>input：a function；</p><p>output：how bad/good it is</p><p>由于，即<code>f</code>是由<code>b</code>和<code>w</code>决定的，因此<code>input f</code>就等价于<code>input</code>这个<code>f</code>里的<code>b</code>和<code>w</code>，因此<font style="background: yellow;">Loss function实际上是在衡量一组参数的好坏</font></p><p>之前提到的model是由我们自主选择的，这里的loss function也是，最常用的方法就是采用类似于方差和的形式来衡量参数的好坏，即预测值与真值差的平方和；这里真正的数值减估测数值的平方，叫做估测误差，Estimation error，将10个估测误差合起来就是loss function</p><script type="math/tex; mode=display">L(f)=L(w,b)=\sum^{10}_ {n=1}(b+w* x_{cp}^n)</script><p>如果$L(f)$越大，说明该function表现得越不好；$L(f)$越小，说明该function表现得越好</p><p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/loss-function.png" class="lazyload" data-srcset="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/loss-function.png" srcset="data:image/png;base64,666" alt="loss-function"></p><h6 id="Loss-function可视化"><a href="#Loss-function可视化" class="headerlink" title="Loss function可视化"></a>Loss function可视化</h6><p>下图中是loss function的可视化，该图中的每一个点都代表一组<code>(w,b)</code>，也就是对应着一个<code>function</code>；而该点的颜色对应着的loss function的结果<code>L(w,b)</code>，它表示该点对应function的表现有多糟糕，颜色越偏红色代表Loss的数值越大，这个function的表现越不好，越偏蓝色代表Loss的数值越小，这个function的表现越好</p><p>比如图中用红色箭头标注的点就代表了b=-180 , w=-2对应的function，即$y=-180-2* x_{cp}$，该点所在的颜色偏向于红色区域，因此这个function的loss比较大，表现并不好</p><p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/loss-figure.png" class="lazyload" data-srcset="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/loss-figure.png" srcset="data:image/png;base64,666" alt="loss-figure"></p><h5 id="Step3：Pick-the-Best-Function"><a href="#Step3：Pick-the-Best-Function" class="headerlink" title="Step3：Pick the Best Function"></a>Step3：Pick the Best Function</h5><p>我们已经确定了loss function，他可以衡量我们的model里面每一个function的好坏，接下来我们要做的事情就是，从这个function set里面，挑选一个最好的function</p><p>挑选最好的function这一件事情，写成formulation/equation的样子如下：</p><p>$f^* =arg min L(f)$，或者是 </p><script type="math/tex; mode=display">w^* ,b^* =arg\ min_ {w,b}\ L(w,b)=arg\ min_ {w,b}\ \sum_ {n=1}^{10}(y^n-(b+w* x^n_ {cp}))^2</script><p>也就是那个使$L(f)=L(w,b)=Loss$最小的$f$或$(w,b)$，就是我们要找的<script type="math/tex">f^*</script>或<script type="math/tex">(w^* ,b^* )</script>(有点像极大似然估计的思想) </p><p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/best-function.png" class="lazyload" data-srcset="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/best-function.png" srcset="data:image/png;base64,666" alt="best-function"></p><p>利用线性代数的知识，可以解得这个closed-form solution，但这里采用的是一种更为普遍的方法——<font style="background: yellow;">gradient descent(梯度下降法)</font></p><h4 id="Gradient-Descent-梯度下降"><a href="#Gradient-Descent-梯度下降" class="headerlink" title="Gradient Descent 梯度下降"></a>Gradient Descent 梯度下降</h4><p>上面的例子比较简单，用线性代数的知识就可以解；但是对于更普遍的问题来说，<font style="background: yellow;">gradient descent的厉害之处在于，只要$L(f)$是可微分的，gradient descent都可以拿来处理这个$f$，找到表现比较好的parameters</font></p><h5 id="单个参数的问题"><a href="#单个参数的问题" class="headerlink" title="单个参数的问题"></a>单个参数的问题</h5><p>以只带单个参数w的Loss Function <code>L(w)</code>为例，首先保证是$L(w)$<strong>可微</strong>的  </p><script type="math/tex; mode=display">w^* =arg\ min_wL(w)</script><p>我们的目标就是找到这个使Loss最小的$w^* $，实际上就是寻找切线L斜率为0的global minima最小值点(注意，存在一些local minima极小值点，其斜率也是0)</p><p>有一个暴力的方法是，穷举所有的w值，去找到使loss最小的<script type="math/tex">w^*</script>，但是这样做是没有效率的；而gradient descent就是用来解决这个效率问题的</p><ul><li><p>首先随机选取一个初始的点<script type="math/tex">w^0</script> (当然也不一定要随机选取，如果有办法可以得到比较接近$w^<em>$的表现得比较好的$w^0$当初始点，可以有效地提高查找$$w^</em> $$的效率)</p></li><li><p>计算在$w=w^0$的位置的微分，即$\frac{dL}{dw}|_{w=w^0}$，几何意义就是切线的斜率</p></li><li><p>如果切线斜率是negative负的，那么就应该使w变大，即往右踏一步；如果切线斜率是positive正的，那么就应该使w变小，即往左踏一步，每一步的步长step size就是w的改变量</p><p>w的改变量step size的大小取决于两件事</p><ul><li><p>一是现在的微分值$\frac{dL}{dw}$有多大，微分值越大代表现在在一个越陡峭的地方，那它要移动的距离就越大，反之就越小；</p></li><li><p>二是一个常数项$\eta$，被称为learning rate，即学习率，它决定了每次踏出的step size不只取决于现在的斜率，还取决于一个事先就定好的数值，如果learning rate比较大，那每踏出一步的时候，参数w更新的幅度就比较大，反之参数更新的幅度就比较小</p><p>如果learning rate设置的大一些，那机器学习的速度就会比较快；但是learning rate如果太大，可能就会跳过最合适的global minima的点</p></li></ul></li><li><p>因此每次参数更新的大小是$\eta\frac{dL}{dw}$ ，为了满足斜率为负时w变大，斜率为正时w变小，应当使原来的w减去更新的数值，即</p><script type="math/tex; mode=display">\begin{bmatrix}w^1=w^0-\eta\frac{dL}{dw}|_ {w=w^0} \tag{1} \\ w^2=w^1-\eta\frac{dL}{dw}|_ {w=w^1} \\ w^3=w^2-\eta\frac{dL}{dw}|_ {w=w^2} \\ ... \\ w^{i+1}=w^i-\eta\frac{dL}{dw}|_ {w=w^i} \\ if(\frac{dL}{dw}|_ {w=w^i}==0)\ then\ stop;\end{bmatrix}</script></li></ul><p>  此时对应的斜率为0，我们找到了一个极小值local minima，这就出现了一个问题，当微分为0的时候，参数就会一直卡在这个点上没有办法再更新了，因此通过gradient descent找出来的solution其实并不是最佳解global minima</p><p>  但幸运的是，在linear regression上，是没有local minima的，因此可以使用这个方法</p><p>  <img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/gradient-descent.png" class="lazyload" data-srcset="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/gradient-descent.png" srcset="data:image/png;base64,666" alt="gradient-descent"></p><h5 id="两个参数的问题"><a href="#两个参数的问题" class="headerlink" title="两个参数的问题"></a>两个参数的问题</h5><p>今天要解决的关于宝可梦的问题，是含有two parameters的问题，即</p><script type="math/tex; mode=display">(w^* ,b^* )=arg\ min_ {w,b}L(w,b)</script><p>当然，它本质上处理单个参数的问题是一样的</p><ul><li><p>首先，也是随机选取两个初始值，$w^0$和$b^0$</p></li><li><p>然后分别计算$(w^0,b^0)$这个点上，L对w和b的偏微分，即$\frac{\partial L}{\partial w}|_ {w=w^0,b=b^0}$ 和 $\frac{\partial L}{\partial b}|_ {w=w^0,b=b^0}$</p></li><li><p>更新参数，当迭代跳出时，$(w^i,b^i)$对应着极小值点</p><script type="math/tex; mode=display">w^1=w^0-\eta \frac{\partial L}{\partial w}|_ {w=w^0,b=b^0} \tag{2}\ \ \ \ \ \ \ \ b^1=b^0-\eta \frac{\partial L}{\partial b}|_ {w=w^0,b=b^0} \\ w^2=w^1-\eta \frac{\partial L}{\partial w}|_ {w=w^1,b=b^1}\ \ \ \ \ \ \ \ b^2=b^1-\eta \frac{\partial L}{\partial b}|_ {w=w^1,b=b^1} \\ ... \\ w^{i+1}=w^i-\eta \frac{\partial L}{\partial w}|_ {w=w^i,b=b^i}\ \ \ \ \ \ \ \ b^{i+1}=b^i-\eta \frac{\partial L}{\partial b}|_ {w=w^i,b=b^i} \\ if(\frac{\partial L}{\partial w}== 0\&\&\frac{\partial L}{\partial b}== 0)\ then\ stop</script></li></ul><p>实际上，L 的gradient就是微积分中的那个梯度的概念，即</p><script type="math/tex; mode=display">\nabla L=\begin{bmatrix}\frac{\partial L}{\partial w} \tag{3} \\ \frac{\partial L}{\partial b}\end{bmatrix}_ {gradient}</script><p>可视化效果如下：(三维坐标显示在二维图像中，loss的值用颜色来表示)</p><p>横坐标是b，纵坐标是w，颜色代表loss的值，越偏蓝色表示loss越小，越偏红色表示loss越大</p><p><strong>每次计算得到的梯度gradient，即由</strong>$\frac{\partial L}{\partial b}$<strong>和</strong>$\frac{\partial L}{\partial w}$<strong>组成的vector向量，就是该等高线的法线方向(对应图中红色箭头的方向)；而</strong>$(-\eta \frac{\partial L}{\partial b},-\eta \frac{\partial L}{\partial w})$<strong>的作用就是让原先的</strong>$(w^i,b^i)$<strong>朝着gradient的方向即等高线法线方向前进，其中η(learning rate)的作用是每次更新的跨度(对应图中红色箭头的长度)；经过多次迭代，最终gradient达到极小值点</strong></p><p>注：这里两个方向的η(learning rate)必须保持一致，这样每次更新坐标的step size是等比例缩放的，保证坐标前进的方向始终和梯度下降的方向一致；否则坐标前进的方向将会发生偏移</p><p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/gradient-two-parameters.png" class="lazyload" data-srcset="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/gradient-two-parameters.png" srcset="data:image/png;base64,666" alt="gradient-two-parameters"></p><h5 id="Gradient-Descent的缺点"><a href="#Gradient-Descent的缺点" class="headerlink" title="Gradient Descent的缺点"></a>Gradient Descent的缺点</h5><p>gradient descent有一个令人担心的地方，也就是我之前一直提到的，它每次迭代完毕，寻找到的梯度为0的点必然是极小值点，local minima；却不一定是最小值点，global minima</p><p>这会造成一个问题是说，如果loss function长得比较坑坑洼洼(极小值点比较多)，而每次初始化$w^0$的取值又是随机的，这会造成每次gradient descent停下来的位置都可能是不同的极小值点；而且当遇到梯度比较平缓(gradient≈0)的时候，gradient descent也可能会效率低下甚至可能会stuck卡住；也就是说通过这个方法得到的结果，是看人品的(滑稽)</p><p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/gradient-stuck.png" class="lazyload" data-srcset="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/gradient-stuck.png" srcset="data:image/png;base64,666" alt="gradient-stuck"></p><p>但是！在<font style="background: yellow;">linear regression</font>里，loss function实际上是<strong>convex</strong>的，是一个<strong>凸函数</strong>，是没有local optimal局部最优解的，他只有一个global minima，visualize出来的图像就是从里到外一圈一圈包围起来的椭圆形的等高线(就像前面的等高线图)，因此随便选一个起始点，根据gradient descent最终找出来的，都会是同一组参数</p><h4 id="回到pokemon的问题上来"><a href="#回到pokemon的问题上来" class="headerlink" title="回到pokemon的问题上来"></a>回到pokemon的问题上来</h4><h5 id="偏微分的计算"><a href="#偏微分的计算" class="headerlink" title="偏微分的计算"></a>偏微分的计算</h5><p>现在我们来求具体的L对w和b的偏微分</p><script type="math/tex; mode=display">L(w,b)=\sum^{10}_ {n=1}(\hat{y}^n-(b+w*x^n_ {cp}))^2 \tag{4} \\\frac{\partial L}{\partial w}=\sum_ {n=1}^{10}2(\hat{y}^n-(b+w*x^n_ {cp}))(-x_ {cp}^n) \\\frac{\partial L}{\partial b}=\sum_ {n=1}^{10}2(\hat{y}^n-(b+w*x^n_ {cp}))(-1)</script><h5 id="How’s-the-results"><a href="#How’s-the-results" class="headerlink" title="How’s the results?"></a>How’s the results?</h5><p>根据gradient descent，我们得到的$y=b+w*x_{cp}$中最好的参数是b=-188.4, w=2.7</p><p>我们需要有一套评估系统来评价我们得到的最后这个function和实际值的误差error的大小；这里我们将training data里每一只宝可梦 $i$ 进化后的实际cp值与预测值之差的绝对值叫做$e^i$，而这些误差之和Average Error on Training Data为$\sum_{i=1}^{10}e^i=31.9$</p><blockquote><p>What we really care about is the error on new data (testing data)</p></blockquote><p>当然我们真正关心的是generalization的case，也就是用这个model去估测新抓到的pokemon，误差会有多少，这也就是所谓的testing data的误差；于是又抓了10只新的pokemon，算出来的Average Error on Testing Data为$\sum_ {i=1}^{10}e^i=35.0$；可见training data里得到的误差一般是要比testing data要小，这也符合常识</p><p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/results.png" class="lazyload" data-srcset="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/results.png" srcset="data:image/png;base64,666" alt="results"></p><h5 id="How-can-we-do-better"><a href="#How-can-we-do-better" class="headerlink" title="How can we do better?"></a>How can we do better?</h5><p>我们有没有办法做得更好呢？这时就需要我们重新去设计model；如果仔细观察一下上图的data，就会发现在原先的cp值比较大和比较小的地方，预测值是相当不准的</p><p>实际上，从结果来看，最终的function可能不是一条直线，可能是稍微更复杂一点的曲线</p><h6 id="考虑的-x-cp-2-model"><a href="#考虑的-x-cp-2-model" class="headerlink" title="考虑的$(x_ {cp})^2$model"></a>考虑的$(x_ {cp})^2$model</h6><p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/Xcp-2.png" class="lazyload" data-srcset="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/Xcp-2.png" srcset="data:image/png;base64,666" alt="Xcp-2"></p><h6 id="考虑的-x-cp-3-model"><a href="#考虑的-x-cp-3-model" class="headerlink" title="考虑的$(x_ {cp})^3$model"></a>考虑的$(x_ {cp})^3$model</h6><p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/Xcp-3.png" class="lazyload" data-srcset="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/Xcp-3.png" srcset="data:image/png;base64,666" alt="Xcp-3"></p><h6 id="考虑的-x-cp-4-model"><a href="#考虑的-x-cp-4-model" class="headerlink" title="考虑的$(x_ {cp})^4$model"></a>考虑的$(x_ {cp})^4$model</h6><p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/Xcp-4.png" class="lazyload" data-srcset="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/Xcp-4.png" srcset="data:image/png;base64,666" alt="Xcp-4"></p><h6 id="考虑的-x-cp-5-model"><a href="#考虑的-x-cp-5-model" class="headerlink" title="考虑的$(x_ {cp})^5$model"></a>考虑的$(x_ {cp})^5$model</h6><p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/Xcp-5.png" class="lazyload" data-srcset="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/Xcp-5.png" srcset="data:image/png;base64,666" alt="Xcp-5"></p><h6 id="5个model的对比"><a href="#5个model的对比" class="headerlink" title="5个model的对比"></a>5个model的对比</h6><p>这5个model的training data的表现：随着$(x_ {cp})^i$的高次项的增加，对应的average error会不断地减小；实际上这件事情非常容易解释，实际上低次的式子是高次的式子的特殊情况(令高次项$(x_ {cp})^i$对应的$w_ i$为0，高次式就转化成低次式)</p><p>也就是说，在gradient descent可以找到best function的前提下(多次式为Non-linear model，存在local optimal局部最优解，gradient descent不一定能找到global minima)，function所包含的项的次数越高，越复杂，error在training data上的表现就会越来越小；但是，我们关心的不是model在training data上的error表现，而是model在testing data上的error表现</p><p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/Xcp-compare.png" class="lazyload" data-srcset="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/Xcp-compare.png" srcset="data:image/png;base64,666" alt="compare"></p><p>在training data上，model越复杂，error就会越低；但是在testing data上，model复杂到一定程度之后，error非但不会减小，反而会暴增，在该例中，从含有$(X_ {cp})^4$项的model开始往后的model，testing data上的error出现了大幅增长的现象，通常被称为<strong>overfitting过拟合</strong></p><p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/Xcp-overfitting.png" class="lazyload" data-srcset="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/Xcp-overfitting.png" srcset="data:image/png;base64,666" alt="overfitting"></p><p>因此model不是越复杂越好，而是选择一个最适合的model，在本例中，包含的$(X_ {cp})^3$式子是最适合的model</p><h5 id="进一步讨论其他参数"><a href="#进一步讨论其他参数" class="headerlink" title="进一步讨论其他参数"></a>进一步讨论其他参数</h5><h6 id="物种-x-s-的影响"><a href="#物种-x-s-的影响" class="headerlink" title="物种$(x_ s)$的影响"></a>物种$(x_ s)$的影响</h6><p>之前我们的model只考虑了宝可梦进化前的cp值，这显然是不对的，除了cp值外，还受到物种$x_ s$的影响</p><p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/hidden-factors.png" class="lazyload" data-srcset="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/hidden-factors.png" srcset="data:image/png;base64,666" alt="hidden-factors"></p><p>因此我们重新设计model：</p><script type="math/tex; mode=display">if\ x_ s=Pidgey:\ \ \ \ \ \ \ y=b_ 1+w_ 1*x_ {cp} \tag{5} \\if\ x_ s=Weedle:\ \ \ \ \ \ y=b_ 2+w_ 2*x_ {cp} \\if\ x_ s=Caterpie:\ \ \ \ y=b_ 3+w_ 3*x_ {cp} \\if\ x_ s=Eevee:\ \ \ \ \ \ \ \ \ y=b_ 4+w_ 4*x_ {cp}</script><p>也就是根据不同的物种，设计不同的linear model(这里$x_ s=species of x$)，那如何将上面的四个if语句合并成一个linear model呢？</p><p>这里引入$\delta$条件表达式的概念，当条件表达式为true，则δ为1；当条件表达式为false，则δ为0，因此可以通过下图的方式，将4个if语句转化成同一个linear model</p><p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/new-model.png" class="lazyload" data-srcset="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/new-model.png" srcset="data:image/png;base64,666" alt="new-model"></p><p>有了上面这个model以后，我们分别得到了在training data和testing data上测试的结果：</p><p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/new-results.png" class="lazyload" data-srcset="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/new-results.png" srcset="data:image/png;base64,666" alt="new-results"></p><h6 id="Hp值-x-hp-、height值-x-g-、weight值-x-w-的影响"><a href="#Hp值-x-hp-、height值-x-g-、weight值-x-w-的影响" class="headerlink" title="Hp值$x_ {hp}$、height值$x_ g$、weight值$x_ w$的影响"></a>Hp值$x_ {hp}$、height值$x_ g$、weight值$x_ w$的影响</h6><p>考虑所有可能有影响的参数，设计出这个最复杂的model：</p><script type="math/tex; mode=display">if\ x_s=Pidgey:\ \ \ \ \ y'=b_1+w_1*x_{cp}+w5*(x_{cp})^2 \tag{6} \\if\ x_s=Weedle:\ \ \ \ y'=b_2+w_2*x_{cp}+w6*(x_{cp})^2 \\if\ x_s=Pidgey:\ \ \ \ \ y'=b_3+w_3*x_{cp}+w7*(x_{cp})^2 \\if\ x_s=Eevee:\ \ \ \ \ \ y'=b_4+w_4*x_{cp}+w8*(x_{cp})^2 \\y=y'+w_9*x_{hp}+w_{10}*(x_{hp})^2+w_{11}*x_h+w_{12}*(x_h)^2+w_{13}*x_w+w_{14}*(x_w)^2</script><p>算出的training error=1.9，但是，testing error=102.3！<strong>这么复杂的model很大概率会发生overfitting</strong>(按照我的理解，overfitting实际上是我们多使用了一些input的变量或是变量的高次项使曲线跟training data拟合的更好，但不幸的是这些项并不是实际情况下被使用的，于是这个model在testing data上会表现得很糟糕)，overfitting就相当于是那个范围更大的韦恩图，它包含了更多的函数更大的范围，代价就是在准确度上表现得更糟糕</p><h6 id="regularization解决overfitting-L2正则化解决过拟合问题"><a href="#regularization解决overfitting-L2正则化解决过拟合问题" class="headerlink" title="regularization解决overfitting(L2正则化解决过拟合问题)"></a>regularization解决overfitting(L2正则化解决过拟合问题)</h6><blockquote><p>regularization可以使曲线变得更加smooth，training data上的error变大，但是 testing data上的error变小。有关regularization的具体原理说明详见下一部分</p></blockquote><p>原来的loss function只考虑了prediction的error，即$\sum_ i^n(\hat{y}^i-(b+\sum_ jw_ jx_ j))^2$；而regularization则是在原来的loss function的基础上加上了一项$\lambda\sum(w_ i)^2$，就是把这个model里面所有的$w_i$的平方和用λ加权(其中i代表遍历n个training data，j代表遍历model的每一项)</p><p>也就是说，<strong>我们期待参数$w_i$越小甚至接近于0的function，为什么呢？</strong></p><p>因为参数值接近0的function，是比较平滑的；所谓的平滑的意思是，当今天的输入有变化的时候，output对输入的变化是比较不敏感的</p><p>举例来说，对$y=b+\sum w_ ix_ i$这个model，当input变化$\Delta x_ i$，output的变化就是$w_ i\Delta x_ i$，也就是说，如果$w_ i$越小越接近0的话，输出对输入就越不sensitive敏感，我们的function就是一个越平滑的function；说到这里你会发现，我们之前没有把bias——b这个参数考虑进去的原因是<strong>bias的大小跟function的平滑程度是没有关系的</strong>，bias值的大小只是把function上下移动而已</p><p><strong>那为什么我们喜欢比较平滑的function呢？</strong></p><p>如果我们有一个比较平滑的function，由于输出对输入是不敏感的，测试的时候，一些noises噪声对这个平滑的function的影响就会比较小，而给我们一个比较好的结果</p><p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/regularization.png" class="lazyload" data-srcset="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/regularization.png" srcset="data:image/png;base64,666" alt="regularization"></p><p><strong>注：这里的λ需要我们手动去调整以取得最好的值</strong></p><p>λ值越大代表考虑smooth的那个regularization那一项的影响力越大，我们找到的function就越平滑</p><p>观察下图可知，当我们的λ越大的时候，在training data上得到的error其实是越大的，但是这件事情是非常合理的，因为当λ越大的时候，我们就越倾向于考虑w的值而越少考虑error的大小；但是有趣的是，虽然在training data上得到的error越大，但是在testing data上得到的error可能会是比较小的</p><p>下图中，当λ从0到100变大的时候，training error不断变大，testing error反而不断变小；但是当λ太大的时候(&gt;100)，在testing data上的error就会越来越大</p><p><font style="background: yellow;">我们喜欢比较平滑的function，因为它对noise不那么sensitive；但是我们又不喜欢太平滑的function，因为它就失去了对data拟合的能力；而function的平滑程度，就需要通过调整λ来决定</font>，就像下图中，当λ=100时，在testing data上的error最小，因此我们选择λ=100</p><p>注：这里的error指的是$\frac{1}{n}\sum_ {i=1}^n|\hat{y}^i-y^i|$</p><p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/regularization-performance.png" class="lazyload" data-srcset="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/regularization-performance.png" srcset="data:image/png;base64,666" alt="regularization-performance"></p><h4 id="conclusion总结"><a href="#conclusion总结" class="headerlink" title="conclusion总结"></a>conclusion总结</h4><h5 id="关于pokemon的cp值预测的流程总结："><a href="#关于pokemon的cp值预测的流程总结：" class="headerlink" title="关于pokemon的cp值预测的流程总结："></a>关于pokemon的cp值预测的流程总结：</h5><ul><li><p>根据已有的data特点(labeled data，包含宝可梦及进化后的cp值)，确定使用supervised learning监督学习</p></li><li><p>根据output的特点(输出的是scalar数值)，确定使用regression回归(linear or non-linear)</p></li><li><p>考虑包括进化前cp值、species、hp等各方面变量属性以及高次项的影响，我们的model可以采用这些input的一次项和二次型之和的形式，如：</p><script type="math/tex; mode=display">if\ x_ s=Pidgey:\ \ \ \ \ y'=b_ 1+w_ 1*x_ {cp}+w5*(x_ {cp})^2 \tag{7} \\if\ x_ s=Weedle:\ \ \ \ y'=b_ 2+w_ 2*x_ {cp}+w6*(x_ {cp})^2 \\if\ x_ s=Pidgey:\ \ \ \ \ y'=b_ 3+w_ 3*x_ {cp}+w7*(x_ {cp})^2 \\if\ x_ s=Eevee:\ \ \ \ \ \ y'=b_ 4+w_ 4*x_ {cp}+w8*(x_ {cp})^2 \\y=y'+w_ 9*x_ {hp}+w_ {10}*(x_ {hp})^2+w_ {11}*x_ h+w_ {12}*(x_ h)^2+w_ {13}*x_ w+w_ {14}*(x_ w)^2</script></li></ul><p>  而为了保证function的平滑性，loss function应使用regularization，即$L=\sum_ {i=1}^n(\hat{y}^i-y^i)^2+\lambda \sum_ j(w_ j)^2$，注意bias——参数b对function平滑性无影响，因此不额外再次计入loss function(y的表达式里已包含w、b)</p><ul><li><p>利用gradient descent对regularization版本的loss function进行梯度下降迭代处理，每次迭代都减去L对该参数的微分与learning rate之积，假设所有参数合成一个vector：$\begin{bmatrix} w_ 0,w_ 1,w_ 2,…w_ j,…,b \end{bmatrix}^T$，那么每次梯度下降的表达式如下：</p><script type="math/tex; mode=display">梯度:\nabla L=\begin{bmatrix}\frac{\partial L}{\partial{w_0}} \\\frac{\partial L}{\partial{w_1}} \\\frac{\partial L}{\partial{w_2}} \\... \\\frac{\partial L}{\partial{w_j}} \\... \\\frac{\partial L}{\partial{b}} \\\end{bmatrix}_{gradient}gradient descent: \begin{bmatrix}w'_0 \\w'_1 \\w'_2 \\... \\w'_j \\... \\b'\end{bmatrix}_{L=L'}=\begin{bmatrix}w_0 \\w_1 \\w_2 \\... \\w_j \\... \\b\end{bmatrix}_{L=L_0}-\begin{bmatrix}\frac{\partial L}{\partial{w_0}} \\\frac{\partial L}{\partial{w_1}} \\\frac{\partial L}{\partial{w_2}} \\... \\\frac{\partial L}{\partial{w_j}} \\... \\\frac{\partial L}{\partial{b}} \\\end{bmatrix}_{L=L_0}\tag{8}</script></li></ul><p>  当梯度稳定不变时，即$\nabla L$为0时，gradient descent便停止，此时如果采用的model是linear的，那么vector必然落于global minima处(凸函数)；如果采用的model是Non-linear的，vector可能会落于local minima处(此时需要采取其他办法获取最佳的function)</p><p>  假定我们已经通过各种方法到达了global minima的地方，此时的vector：$\begin{bmatrix}w_ 0,w_ 1,w_ 2,…,w_ j,…b\end{bmatrix}^T$所确定的那个唯一的function就是在该λ下的最佳$f^*$，即loss最小</p><ul><li><p>这里λ的最佳数值是需要通过我们不断调整来获取的，因此令λ等于0，10，100，1000，…不断使用gradient descent或其他算法得到最佳的parameters：$\begin{bmatrix}w_ 0,w_ 1,w_ 2,…,w_ j,…b\end{bmatrix}^T$，并计算出这组参数确定的function——$f^* $对training data和testing data上的error值，直到找到那个使testing data的error最小的λ，(这里一开始λ=0，就是没有使用regularization时的loss function)</p><p>注：引入评价<script type="math/tex">f^*</script>的error机制，令error=$\frac{1}{n}\sum_ {i=1}{n}|\hat{y}^i-y^i|$，分别计算该<script type="math/tex">f^*</script>对training data和testing data(more important)的<script type="math/tex">error(f^* )</script>大小</p><blockquote><p>先设定λ-&gt;确定loss function-&gt;找到使loss最小的$\begin{bmatrix}w_ 0,w_ 1,w_ 2,…,w_ j,…b\end{bmatrix}^T$-&gt;确定function-&gt;计算error-&gt;重新设定新的λ重复上述步骤-&gt;使testing data上的error最小的λ所对应的$\begin{bmatrix}w_ 0,w_ 1,w_ 2,…,w_ j,…b\end{bmatrix}^T$所对应的function就是我们能够找到的最佳的function</p></blockquote></li></ul><h5 id="本章节总结："><a href="#本章节总结：" class="headerlink" title="本章节总结："></a>本章节总结：</h5><ul><li><p>Pokémon: Original CP and species almost decide the CP after evolution </p></li><li><p>There are probably other hidden factors</p></li><li><p>Gradient descent</p><ul><li>More theory and tips in the following lectures </li></ul></li><li><p>Overfitting and Regularization</p></li><li><p>We finally get average error = 11.1 on the testing data</p></li><li><p>How about new data? Larger error? Lower error?(larger-&gt;need validation)</p></li><li><p>Next lecture: Where does the error come from?</p><ul><li>More theory about overfitting and regularization</li><li>The concept of validation(用来解决new data的error高于11.1的问题)</li></ul></li></ul><h4 id="附：Regularization-L1-L2-正则化解决overfitting"><a href="#附：Regularization-L1-L2-正则化解决overfitting" class="headerlink" title="附：Regularization(L1 L2 正则化解决overfitting)"></a>附：Regularization(L1 L2 正则化解决overfitting)</h4><blockquote><p>Regularization -&gt; redefine the loss function</p></blockquote><p>关于overfitting的问题，很大程度上是由于曲线为了更好地拟合training data的数据，而引入了更多的高次项，使得曲线更加“蜿蜒曲折”，反而导致了对testing data的误差更大</p><p>回过头来思考，我们之前衡量model中某个function的好坏所使用的loss function，仅引入了真实值和预测值差值的平方和这一个衡量标准；我们想要避免overfitting过拟合的问题，就要使得高次项对曲线形状的影响尽可能小，因此我们要在loss function里引入高次项(非线性部分)的衡量标准，也就是将高次项的系数也加权放进loss function中，这样可以使得训练出来的model既满足预测值和真实值的误差小，又满足高次项的系数尽可能小而使曲线的形状比较稳定集中</p><p>以下图为例，如果loss function仅考虑了$(\hat{y}-y)^2$这一误差衡量标准，那么拟合出来的曲线就是红色虚线部分(过拟合)，而过拟合就是所谓的model对training data过度自信, 非常完美的拟合上了这些数据, 如果具备过拟合的能力, 那么这个方程就可能是一个比较复杂的非线性方程 , 正是因为这里的$x^3$和$x^2$使得这条虚线能够被弯来弯去, 所以整个模型就会特别努力地去学习作用在$x^3$和$x^2$上的c、d参数. <strong>但是在这个例子里，我们期望模型要学到的却是这条蓝色的曲线. 因为它能更有效地概括数据</strong>.而且只需要一个$y=a+bx$就能表达出数据的规律. </p><p>或者是说, 蓝色的线最开始时, 和红色线同样也有c、d两个参数, 可是最终学出来时, c 和 d 都学成了0, 虽然蓝色方程的误差要比红色大, 但是概括起数据来还是蓝色好</p><p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/L1L2regularization.png" class="lazyload" data-srcset="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/L1L2regularization.png" srcset="data:image/png;base64,666" alt="regularization"></p><p>这也是我们通常采用的方法，我们不可能一开始就否定高次项而直接只采用低次线性表达式的model，因为有时候真实数据的确是符合高次项非线性曲线的分布的；而如果一开始直接采用高次非线性表达式的model，就很有可能造成overfitting，在曲线偏折的地方与真实数据的误差非常大。我们的目标应该是这样的：</p><p><strong>在无法确定真实数据分布的情况下，我们尽可能去改变loss function的评价标准</strong></p><ul><li><strong>我们的model的表达式要尽可能的复杂，包含尽可能多的参数和尽可能多的高次非线性项；</strong></li><li><strong>但是我们的loss function又有能力去控制这条曲线的参数和形状，使之不会出现overfitting过拟合的现象；</strong></li><li><strong>在真实数据满足高次非线性曲线分布的时候，loss function控制训练出来的高次项的系数比较大，使得到的曲线比较弯折起伏；</strong></li><li><strong>在真实数据满足低次线性分布的时候，loss function控制训练出来的高次项的系数比较小甚至等于0，使得到的曲线接近linear分布</strong></li></ul><p>那我们如何保证能学出来这样的参数呢? 这就是 L1 L2 正规化出现的原因.</p><p>之前的loss function仅考虑了$(\hat{y}-y)^2$这一误差衡量标准，而<strong>L1 L2正规化</strong>就是在这个loss function的后面多加了一个东西，即model中跟高次项系数有关的表达式；</p><ul><li>L1正规化即加上$\lambda\sum|w_ j|$这一项，loss function变成$L=\sum_ {i=1}^n(\hat{y}^i-y^i)^2+\lambda\sum|w_ j|$，即n个training data里的数据的真实值与预测值差值的平方和加上λ权重下的model表达式中所有项系数的绝对值之和</li><li>L2正规化即加上$\lambda\sum(w_ j)^2$这一项，loss function变成$L=\sum_ {i=1}^n(\hat{y}^i-y^i)^2+\lambda\sum(w_ j)^2$，即n个training data里的数据的真实值与预测值差值的平方和加上λ权重下的model表达式中所有项系数的平方和</li></ul><p>相对来说，L2要更稳定一些，L1的结果则不那么稳定，如果用p表示正规化程度，上面两式可总结如下：</p><script type="math/tex; mode=display">L=\sum_ {i=1}^n(\hat{y}^i-y^i)+\lambda\sum_ j(w_ j)^p</script><p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/L1-L2.png" class="lazyload" data-srcset="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/L1-L2.png" srcset="data:image/png;base64,666" alt="L1-L2"></p>]]></content>
    
    <summary type="html">
    
      Regression - Case Study
    
    </summary>
    
    
      <category term="Regression - Case Study" scheme="http://extheor.club/volantis/categories/regression-case-study/"/>
    
    
  </entry>
  
  <entry>
    <title>哔哔！换个姿势在静态博客上发短博文（volantis适配版）</title>
    <link href="http://extheor.club/volantis/blogs/2020-10-09-bb-on-volantis/"/>
    <id>http://extheor.club/volantis/blogs/2020-10-09-bb-on-volantis/</id>
    <published>2020-10-08T16:00:00.000Z</published>
    <updated>2020-10-08T16:00:00.000Z</updated>
    
    <summary type="html">
    
      本文教你如何用微信在Hexo上发表类似于说说的短博文。
    
    </summary>
    
    
      <category term="进阶玩法" scheme="http://extheor.club/volantis/categories/%E8%BF%9B%E9%98%B6%E7%8E%A9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>Hexo 标签函数 list_tags 用法详解</title>
    <link href="http://extheor.club/volantis/blogs/2020-09-24-list-tags/"/>
    <id>http://extheor.club/volantis/blogs/2020-09-24-list-tags/</id>
    <published>2020-09-23T16:00:00.000Z</published>
    <updated>2020-09-23T16:00:00.000Z</updated>
    
    <summary type="html">
    
      对官方文档标签文档的一个补充，以volantis主题标签调用为例，展示不同用法下的标签示例。
    
    </summary>
    
    
      <category term="开发心得" scheme="http://extheor.club/volantis/categories/%E5%BC%80%E5%8F%91%E5%BF%83%E5%BE%97/"/>
    
    
  </entry>
  
  <entry>
    <title>如何给博客添加弹窗通知</title>
    <link href="http://extheor.club/volantis/blogs/2020-09-03-message-prompt/"/>
    <id>http://extheor.club/volantis/blogs/2020-09-03-message-prompt/</id>
    <published>2020-09-02T16:00:00.000Z</published>
    <updated>2020-09-03T16:00:00.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
      <category term="进阶玩法" scheme="http://extheor.club/volantis/categories/%E8%BF%9B%E9%98%B6%E7%8E%A9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>博客访问速度提升：最佳线路分流</title>
    <link href="http://extheor.club/volantis/blogs/2020-08-31-hexospeed/"/>
    <id>http://extheor.club/volantis/blogs/2020-08-31-hexospeed/</id>
    <published>2020-08-30T16:00:00.000Z</published>
    <updated>2020-08-30T16:00:00.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
      <category term="进阶玩法" scheme="http://extheor.club/volantis/categories/%E8%BF%9B%E9%98%B6%E7%8E%A9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>静态博客使用 Issues API 发布动态、友链、书签</title>
    <link href="http://extheor.club/volantis/blogs/2020-08-28-issue-api/"/>
    <id>http://extheor.club/volantis/blogs/2020-08-28-issue-api/</id>
    <published>2020-08-27T16:00:00.000Z</published>
    <updated>2020-08-27T16:00:00.000Z</updated>
    
    <summary type="html">
    
      由于发布 issue 的成本远远低于发布一次博客更新（即便是使用了持续集成），可以用 issue 来简化每个独立博客都必备的友链系统，也可以通过 issue 来快速发布动态资讯，弥补静态博客必须更新静态文件才能更新内容的缺点。此功能已经集成到了主题中，使用非常方便。
    
    </summary>
    
    
      <category term="进阶玩法" scheme="http://extheor.club/volantis/categories/%E8%BF%9B%E9%98%B6%E7%8E%A9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>给 Volantis 主题调用 Valine 最新评论</title>
    <link href="http://extheor.club/volantis/blogs/2020-09-03-latest-comments/"/>
    <id>http://extheor.club/volantis/blogs/2020-09-03-latest-comments/</id>
    <published>2020-08-27T16:00:00.000Z</published>
    <updated>2020-09-02T16:00:00.000Z</updated>
    
    <summary type="html">
    
      给博客加上最新评论栏目。以 Volantis 主题为例，给侧边栏加上最新评论功能，实现整站调用。
    
    </summary>
    
    
      <category term="进阶玩法" scheme="http://extheor.club/volantis/categories/%E8%BF%9B%E9%98%B6%E7%8E%A9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>给 Hexo Volantis 主题添加图片轮播功能</title>
    <link href="http://extheor.club/volantis/blogs/2020-08-21-main-banner/"/>
    <id>http://extheor.club/volantis/blogs/2020-08-21-main-banner/</id>
    <published>2020-08-20T16:00:00.000Z</published>
    <updated>2020-08-21T16:00:00.000Z</updated>
    
    <summary type="html">
    
      很多网站首页都会有图片轮播效果，给网站的首页加上图片轮播的效果，可以很好的起到广告的作用也可以起到推荐优秀内容的作用。来吧，下面是给 Volantis 主题加上首页图片轮播的效果。
    
    </summary>
    
    
      <category term="进阶玩法" scheme="http://extheor.club/volantis/categories/%E8%BF%9B%E9%98%B6%E7%8E%A9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>如何快速优雅地删除 Valine 的垃圾评论</title>
    <link href="http://extheor.club/volantis/blogs/2020-07-03-valine/"/>
    <id>http://extheor.club/volantis/blogs/2020-07-03-valine/</id>
    <published>2020-07-02T16:00:00.000Z</published>
    <updated>2020-11-13T08:51:58.862Z</updated>
    
    <content type="html"><![CDATA[<p>Valine 评论由于可以匿名，发言成本低、质量低，使用 app 可以方便的删除垃圾评论。</p><span id="more"></span><h2 id="操作演示"><a href="#操作演示" class="headerlink" title="操作演示"></a>操作演示</h2><div class="videos center" col='3'><div class="video"><video controls preload><source src='https://github.com/volantis-x/volantis-docs/releases/download/assets/valine-ios.mov' type='video/mp4'>Your browser does not support the video tag.</video></div></div><blockquote><p>App 只支持数据存储于 LeanCloud 的评论系统。</p></blockquote><p>App 登录前需要配置 AppID 和 AppKey，由于字符比较长，输入不方便，所以支持 URL Scheme 配置。将 URL 生成一个二维码，然后用手机扫码就可以打开 App 并自动配置好。</p><p>格式为：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">valine:&#x2F;&#x2F;cfg&#x2F;id&#x3D;7yIoRlSmfX09vQCERsuWzFnx-MdYXbMMI&amp;key&#x3D;3zCL5GFePTUjwbqLop44QFbr&amp;alias&#x3D;测试项目</span><br></pre></td></tr></table></figure><p>也可以把管理员用户的账号和密码配置上，扫码直接登录，但是注意不要泄露出去：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">valine:&#x2F;&#x2F;cfg&#x2F;id&#x3D;7yIoRlSmfX09vQCERsuWzFnx-MdYXbMMI&amp;key&#x3D;3zCL5GFePTUjwbqLop44QFbr&amp;alias&#x3D;测试项目&amp;user&#x3D;me@xaoxuu.com&amp;psw&#x3D;q</span><br></pre></td></tr></table></figure><p>注意：这个管理员用户并不是 LeanCloud 的账号，而是当前 Valine 数据库中的 <code>_User</code> 表中的一个用户，可以在 App 中注册，然后在 LeanCloud 上把注册的用户设置为管理员。</p><p>设置管理员的方法：在 <code>_Role</code> 表中新建一个 <code>admin</code> 角色，然后在 <code>admin</code> 角色的 <code>users</code> 列中点击 <code>Relations</code> 把自己刚注册的用户添加进去，这个用户就有了修改和删除评论数据的权限。</p><h2 id="App-测试版地址"><a href="#App-测试版地址" class="headerlink" title="App 测试版地址"></a>App 测试版地址</h2><div class="btns circle center grid4">            <a href='https://testflight.apple.com/join/zA4MOzDd'>  <i class='fab fa-apple'></i>  <b>Valine for iOS</b>  <img src='https://i.loli.net/2020/07/04/cGmgsHhwlt4vYyK.png'></a><a href='https://github.com/yinhanlei/Valine-Android/releases/'>  <i class='fab fa-android'></i>  <b>Valine for Android</b>  <img src='https://i.loli.net/2020/07/21/HN2UhFIys6gKXct.png'></a>          </div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Valine 评论由于可以匿名，发言成本低、质量低，使用 app 可以方便的删除垃圾评论。&lt;/p&gt;
    
    </summary>
    
    
      <category term="进阶玩法" scheme="http://extheor.club/volantis/categories/%E8%BF%9B%E9%98%B6%E7%8E%A9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>Volantis 主题部署 Pjax</title>
    <link href="http://extheor.club/volantis/blogs/2020-05-17-pjax/"/>
    <id>http://extheor.club/volantis/blogs/2020-05-17-pjax/</id>
    <published>2020-05-16T16:00:00.000Z</published>
    <updated>2020-08-06T16:00:00.000Z</updated>
    
    <summary type="html">
    
      本篇文章记录了我对 Volantis 主题做 Pjax 兼容的种种，大抵算是种记录吧~
    
    </summary>
    
    
      <category term="开发心得" scheme="http://extheor.club/volantis/categories/%E5%BC%80%E5%8F%91%E5%BF%83%E5%BE%97/"/>
    
    
  </entry>
  
  <entry>
    <title>关于主题与文档的更新</title>
    <link href="http://extheor.club/volantis/news/2020-04-04/"/>
    <id>http://extheor.club/volantis/news/2020-04-04/</id>
    <published>2020-04-03T16:00:00.000Z</published>
    <updated>2020-11-13T08:51:58.867Z</updated>
    
    <content type="html"><![CDATA[<p>由于主题目前仍处于青少年阶段，更新迭代速度比较快，所以不会保留旧版本的文档，如果需要查看旧版本的文档，请下载主题文档的源码，回退到旧版本，本地运行查看。</p><ul><li>计划 <code>2.x</code> 的最后一个版本的文档会保留至 <code>5.0</code> 发布时，即最终会同时维护3个大版本的文档。</li><li>当主题稳定后会提供英文文档。</li></ul><span id="more"></span>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;由于主题目前仍处于青少年阶段，更新迭代速度比较快，所以不会保留旧版本的文档，如果需要查看旧版本的文档，请下载主题文档的源码，回退到旧版本，本地运行查看。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计划 &lt;code&gt;2.x&lt;/code&gt; 的最后一个版本的文档会保留至 &lt;code&gt;5.0&lt;/code&gt; 发布时，即最终会同时维护3个大版本的文档。&lt;/li&gt;
&lt;li&gt;当主题稳定后会提供英文文档。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="官方资讯" scheme="http://extheor.club/volantis/categories/%E5%AE%98%E6%96%B9%E8%B5%84%E8%AE%AF/"/>
    
    
  </entry>
  
  <entry>
    <title>新版本「2.0」正式版发布</title>
    <link href="http://extheor.club/volantis/news/2020-03-10-2.0/"/>
    <id>http://extheor.club/volantis/news/2020-03-10-2.0/</id>
    <published>2020-03-09T16:00:00.000Z</published>
    <updated>2020-11-13T08:51:58.867Z</updated>
    
    <content type="html"><![CDATA[<p>本次更新内容非常多，主要升级了导航栏、使用新的语言重写了全部样式。旧版本 Volantis 用户，请您卸载掉旧的样式渲染插件，并安装新的：</p><ol><li><p>卸载 <code>less</code></p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-less --save</span><br></pre></td></tr></table></figure></li><li><p>安装 <code>stylus</code></p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">npm install hexo-renderer-stylus --save</span><br></pre></td></tr></table></figure></li></ol><span id="more"></span><h2 id="新功能"><a href="#新功能" class="headerlink" title="新功能"></a>新功能</h2><h3 id="导航栏"><a href="#导航栏" class="headerlink" title="导航栏"></a>导航栏</h3><p>导航栏获得了史诗级的增强，可以设置<u><strong>N级</strong></u>子菜单。考虑到实用性，不太建议使用过多或者过深的菜单（超出屏幕的部分无法被看到）。</p><h3 id="样式设置"><a href="#样式设置" class="headerlink" title="样式设置"></a>样式设置</h3><p>现在可以在主题配置文件中设置更多的样式：</p><ul><li>最大布局宽度</li><li>导航栏高度、特效（阴影、毛玻璃、鼠标hover时浮起）</li><li>卡片特效（阴影、毛玻璃、鼠标hover时浮起）</li><li>代码框是否显示语言</li><li>标题和正文文本布局（靠左、靠右、居中）</li><li>正文字体</li><li>代码字体</li><li>各部分颜色</li></ul><h3 id="封面"><a href="#封面" class="headerlink" title="封面"></a>封面</h3><p>封面可以在主题配置文件中设置在首页、归档页面、其它页面默认是否显示。<br>封面中可以同时显示logo图片、标题、副标题了。</p><h3 id="小部件"><a href="#小部件" class="headerlink" title="小部件"></a>小部件</h3><p>grid小部件可以设置 <code>fixed: true</code> 来固定网格宽度（适合文字长短不一的场景）。</p><h3 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h3><p>可以设置「评论」的标题和副标题。<br>可以创建多种颜色的折叠框。<br>友链增加分组描述。</p><h2 id="功能调整与优化"><a href="#功能调整与优化" class="headerlink" title="功能调整与优化"></a>功能调整与优化</h2><h3 id="样式渲染器"><a href="#样式渲染器" class="headerlink" title="样式渲染器"></a>样式渲染器</h3><p>使用 <code>stylus</code> 重写了所有样式，无需安装<code>less</code>插件了。相较于<code>1.7.4</code>，<code>css</code>文件体积缩小了<u>19.75%</u>。</p><h3 id="主题配置文件"><a href="#主题配置文件" class="headerlink" title="主题配置文件"></a>主题配置文件</h3><ul><li>主题配置文件经过了较大改动，使得层级结构更加清晰。</li><li>优化了二维码（微信）分享的使用体验</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次更新内容非常多，主要升级了导航栏、使用新的语言重写了全部样式。旧版本 Volantis 用户，请您卸载掉旧的样式渲染插件，并安装新的：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;卸载 &lt;code&gt;less&lt;/code&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight sh&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;npm uninstall hexo-renderer-less --save&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;安装 &lt;code&gt;stylus&lt;/code&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight sh&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;npm install hexo-renderer-stylus --save&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="官方资讯" scheme="http://extheor.club/volantis/categories/%E5%AE%98%E6%96%B9%E8%B5%84%E8%AE%AF/"/>
    
    
  </entry>
  
  <entry>
    <title>新版本「2.0」测试版发布</title>
    <link href="http://extheor.club/volantis/news/2020-03-06-2.0-beta/"/>
    <id>http://extheor.club/volantis/news/2020-03-06-2.0-beta/</id>
    <published>2020-03-06T11:43:00.000Z</published>
    <updated>2020-11-13T08:51:58.867Z</updated>
    
    <content type="html"><![CDATA[<details open><summary> Beta6 </summary>              <div class='content'>              <ul><li>[x] 导航栏N级菜单支持分割线和小标题，详见我的个人博客。</li></ul>              </div>            </details><span id="more"></span><details ><summary> Beta5 </summary>              <div class='content'>              <ul><li>[x] 导航栏<s>二级</s><u>N级</u>菜单</li><li>[x] 支持微信二维码分享（需安装二维码生成插件）</li></ul>              </div>            </details><details ><summary> Beta4 </summary>              <div class='content'>              <ul><li>[x] 优化样式</li><li>[x] 更多样式可以在主题配置文件中自定义</li><li>[x] 调整主题配置文件</li></ul>              </div>            </details><details ><summary> Beta3 </summary>              <div class='content'>              <ul><li>[x] 优化样式</li><li>[x] grid 部件新增 <code>fixed: true</code> 参数，用于固定宽度。</li></ul>              </div>            </details><details ><summary> Beta2 </summary>              <div class='content'>              <ul><li>[x] 可以创建多种颜色的折叠框</li><li>[x] 友链增加分组描述</li></ul>              </div>            </details><details ><summary> Beta1 </summary>              <div class='content'>              <ul><li>[x] 新版本使用 <code>stylus</code> 完全重写了样式。</li><li>[x] 可在主题配置文件中修改配色、标题等多种样式（需要关闭CDN）。<br>已知的BUG有：</li><li>[ ] Container左侧的图标（图片）始终显示不出来，原因未知。</li></ul><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">&gt;<span class="selector-tag">div</span>.info</span><br><span class="line">  <span class="attribute">background-color</span>: alpha(<span class="variable">$color</span>-mac-cyan, <span class="number">20%</span>)</span><br><span class="line">  <span class="attribute">border-left</span>: <span class="variable">$borderradius</span>-codeblock solid <span class="variable">$color</span>-mac-cyan</span><br><span class="line">  <span class="attribute">border-radius</span>: <span class="variable">$borderradius</span>-codeblock</span><br><span class="line">  &gt;:before</span><br><span class="line">    ...(省略无关代码)</span><br><span class="line">    <span class="attribute">background-size</span>: <span class="number">16px</span> <span class="number">16px</span></span><br><span class="line">    <span class="attribute">background-position</span>: <span class="number">4px</span> <span class="number">4px</span></span><br><span class="line">    <span class="attribute">background-repeat</span>: no-repeat</span><br><span class="line">    <span class="attribute">background-color</span>: <span class="variable">$color</span>-mac-cyan</span><br><span class="line">    <span class="attribute">background-image</span>: url(<span class="string">&quot;data:image/svg+xmlbase64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz48c3ZnIHdpZHRoPSIxNzkyIiBoZWlnaHQ9IjE3OTIiIHZpZXdCb3g9IjAgMCAxNzkyIDE3OTIiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+PHBhdGggZD0iTTEyMTYgMTM0NHYxMjhxMCAyNi0xOSA0NXQtNDUgMTloLTUxMnEtMjYgMC00NS0xOXQtMTktNDV2LTEyOHEwLTI2IDE5LTQ1dDQ1LTE5aDY0di0zODRoLTY0cS0yNiAwLTQ1LTE5dC0xOS00NXYtMTI4cTAtMjYgMTktNDV0NDUtMTloMzg0cTI2IDAgNDUgMTl0MTkgNDV2NTc2aDY0cTI2IDAgNDUgMTl0MTkgNDV6bS0xMjgtMTE1MnYxOTJxMCAyNi0xOSA0NXQtNDUgMTloLTI1NnEtMjYgMC00NS0xOXQtMTktNDV2LTE5MnEwLTI2IDE5LTQ1dDQ1LTE5aDI1NnEyNiAwIDQ1IDE5dDE5IDQ1eiIgZmlsbD0iI2ZmZiIvPjwvc3ZnPg==&quot;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>我试过把url里面的内容换成网络图片的URL是可以正常显示的。<br>源码在： <code>themes/volantis/source/css/_third-party/container.styl</code></p></blockquote>              </div>            </details>]]></content>
    
    <summary type="html">
    
      &lt;details open&gt;&lt;summary&gt; Beta6 &lt;/summary&gt;
              &lt;div class=&#39;content&#39;&gt;
              &lt;ul&gt;&lt;li&gt;[x] 导航栏N级菜单支持分割线和小标题，详见我的个人博客。&lt;/li&gt;&lt;/ul&gt;
              &lt;/div&gt;
            &lt;/details&gt;
    
    </summary>
    
    
      <category term="官方资讯" scheme="http://extheor.club/volantis/categories/%E5%AE%98%E6%96%B9%E8%B5%84%E8%AE%AF/"/>
    
    
  </entry>
  
  <entry>
    <title>测试「文章内链接作文本的话无法生成静态文件」</title>
    <link href="http://extheor.club/volantis/test/2020-03-06-test-url/"/>
    <id>http://extheor.club/volantis/test/2020-03-06-test-url/</id>
    <published>2020-03-05T16:00:00.000Z</published>
    <updated>2020-11-13T08:51:58.867Z</updated>
    
    <content type="html"><![CDATA[<p>5、测试Tomcat是否配置成功：打开浏览器，输入在地址栏中输入： <a href="http://localhost:8080">http://localhost:8080</a> 的运行结果如下图即为即为配置成功！（因为Tomcat已经在运行再次打开会报错）</p><p>相关 Issue： <a href="https://github.com/volantis-x/hexo-theme-volantis/issues/164">#164</a></p><div class="note success"><p>结论：存在这篇文章的情况下能够成功 deploy ，说明这不是主题的 BUG。</p></div><div class="note "><p><code>markdwon</code> 解析插件为 <code>hexo-renderer-marked</code> 时此插件默认会自动识别 URL ，且原文链接前后没有空格，所以链接前后增加一个空格或者在根目录配置文件中设置 <code>autolink: false</code> 都可以避免 deploy 报错。<br>插件：<a href="https://github.com/hexojs/hexo-renderer-marked">hexo-renderer-marked</a></p></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;5、测试Tomcat是否配置成功：打开浏览器，输入在地址栏中输入： &lt;a href=&quot;http://localhost:8080&quot;&gt;http://localhost:8080&lt;/a&gt; 的运行结果如下图即为即为配置成功！（因为Tomcat已经在运行再次打开会报错）&lt;/p&gt;
&lt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>新增 pjax 开发分支</title>
    <link href="http://extheor.club/volantis/news/2020-02-25/"/>
    <id>http://extheor.club/volantis/news/2020-02-25/</id>
    <published>2020-02-24T16:00:00.000Z</published>
    <updated>2020-11-13T08:51:58.866Z</updated>
    
    <content type="html"><![CDATA[<p>由于功能未完全兼容，如果想率先使用，尽量不要改主题配置文件。</p><ul><li>不能使用封面</li><li>mathjax</li><li>有折叠框的页面，折叠框打开之后目录高亮位置不对应</li><li>很多未知问题</li></ul><blockquote><p>感谢 <a href="https://inkss.cn/">@inkss</a> 发布的这篇教程 <a href="https://inkss.cn/article/other/80b5f235.html">《Hexo 博客部署 Pjax 局部刷新》</a><br>当所有兼容性问题解决或妥协之后，pjax 分支会合并入 master。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;由于功能未完全兼容，如果想率先使用，尽量不要改主题配置文件。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不能使用封面&lt;/li&gt;
&lt;li&gt;mathjax&lt;/li&gt;
&lt;li&gt;有折叠框的页面，折叠框打开之后目录高亮位置不对应&lt;/li&gt;
&lt;li&gt;很多未知问题&lt;/li&gt;
&lt;/ul&gt;
&lt;blockqu
      
    
    </summary>
    
    
      <category term="官方资讯" scheme="http://extheor.club/volantis/categories/%E5%AE%98%E6%96%B9%E8%B5%84%E8%AE%AF/"/>
    
    
  </entry>
  
  <entry>
    <title>主题更名为「Volantis」</title>
    <link href="http://extheor.club/volantis/news/2020-02-22/"/>
    <id>http://extheor.club/volantis/news/2020-02-22/</id>
    <published>2020-02-21T16:00:00.000Z</published>
    <updated>2020-11-13T08:51:58.866Z</updated>
    
    <content type="html"><![CDATA[<p>主题原名「Material X」,最初由「Material-Flow」改编，现已完全没有材质化设计的影子了。风格偏向简约风技术类博客，最大的特点是自由。现更名为「Volantis」，取自权力的游戏地名。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;主题原名「Material X」,最初由「Material-Flow」改编，现已完全没有材质化设计的影子了。风格偏向简约风技术类博客，最大的特点是自由。现更名为「Volantis」，取自权力的游戏地名。&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="官方资讯" scheme="http://extheor.club/volantis/categories/%E5%AE%98%E6%96%B9%E8%B5%84%E8%AE%AF/"/>
    
    
  </entry>
  
  <entry>
    <title>主题文档网页更新并开源</title>
    <link href="http://extheor.club/volantis/news/2020-02-21/"/>
    <id>http://extheor.club/volantis/news/2020-02-21/</id>
    <published>2020-02-20T16:00:00.000Z</published>
    <updated>2020-11-13T08:51:58.866Z</updated>
    
    <content type="html"><![CDATA[<p>主题经过几年迭代，现已高度模块化和可定制化，功能相对完善，适合用作文档。原来的 <a href="https://github.com/xaoxuu/hexo-theme-vuex">文档主题</a> 将不再维护和更新。</p><p>现在使用的文档页面源码是： <span class='btn'><a class="button" href='https://github.com/volantis-x/volantis-docs' title='volantis-docs'>volantis-docs</a></span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;主题经过几年迭代，现已高度模块化和可定制化，功能相对完善，适合用作文档。原来的 &lt;a href=&quot;https://github.com/xaoxuu/hexo-theme-vuex&quot;&gt;文档主题&lt;/a&gt; 将不再维护和更新。&lt;/p&gt;
&lt;p&gt;现在使用的文档页面源码是： &lt;span
      
    
    </summary>
    
    
      <category term="官方资讯" scheme="http://extheor.club/volantis/categories/%E5%AE%98%E6%96%B9%E8%B5%84%E8%AE%AF/"/>
    
    
  </entry>
  
  <entry>
    <title>已关闭评论区</title>
    <link href="http://extheor.club/volantis/news/2020-02-20/"/>
    <id>http://extheor.club/volantis/news/2020-02-20/</id>
    <published>2020-02-19T16:00:00.000Z</published>
    <updated>2020-11-13T08:51:58.865Z</updated>
    
    <content type="html"><![CDATA[<h4 id="使用-GitHub-Issue"><a href="#使用-GitHub-Issue" class="headerlink" title="使用 GitHub Issue"></a>使用 GitHub Issue</h4><p>由于 Valine 匿名评论不适合追踪和解决问题，因此决定暂时关闭评论区。<br>为了精准高效解决问题，请前往 <span class='btn'><a class="button" href='https://github.com/volantis-x/hexo-theme-volantis/issues/' title='GitHub Issue'>GitHub Issue</a></span></p><h4 id="遇到问题怎么办"><a href="#遇到问题怎么办" class="headerlink" title="遇到问题怎么办"></a>遇到问题怎么办</h4><ol><li>确定已经查阅文档找不到相关内容。</li><li>前往「常见问题」页面，查看是否有解决方案。</li><li>访问在线示例，查看是否具有相同第问题。<br>3.1. 如果在线示例表示正常，则说明自己配置有误，检查开发环境、主题配置是否正确。如果检查不出问题，提Issue询问。<br>3.2. 如果在线示例也存在相同问题，则说明存在BUG，请提Issue反馈。</li><li>如果在线示例没有相关内容，下载示例博客源码，修改运行本地预览，进行第3步判断操作。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;使用-GitHub-Issue&quot;&gt;&lt;a href=&quot;#使用-GitHub-Issue&quot; class=&quot;headerlink&quot; title=&quot;使用 GitHub Issue&quot;&gt;&lt;/a&gt;使用 GitHub Issue&lt;/h4&gt;&lt;p&gt;由于 Valine 匿名评论不适合追
      
    
    </summary>
    
    
      <category term="官方资讯" scheme="http://extheor.club/volantis/categories/%E5%AE%98%E6%96%B9%E8%B5%84%E8%AE%AF/"/>
    
    
  </entry>
  
  <entry>
    <title>版本命名规范化</title>
    <link href="http://extheor.club/volantis/news/2020-02-19/"/>
    <id>http://extheor.club/volantis/news/2020-02-19/</id>
    <published>2020-02-18T16:00:00.000Z</published>
    <updated>2020-11-13T08:51:58.865Z</updated>
    
    <content type="html"><![CDATA[<p class='p red bold'>主版本号.子版本号.修订版本号</p><p>主题从2017年至今，已经经历了相当多的大版本迭代和数不尽的小版本更新，但是版本号的更新一直没有遵循规范。从下次更新开始，将遵循下述的规范：</p><ul><li><strong>主版本号</strong>： 较大改动、框架调整或重构</li><li><strong>子版本号</strong>： 较小或局部的功能性更新</li><li><strong>修订版本号</strong>： 修复BUG或无关紧要的细节调整</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p class=&#39;p red bold&#39;&gt;主版本号.子版本号.修订版本号&lt;/p&gt;
&lt;p&gt;主题从2017年至今，已经经历了相当多的大版本迭代和数不尽的小版本更新，但是版本号的更新一直没有遵循规范。从下次更新开始，将遵循下述的规范：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;主版本
      
    
    </summary>
    
    
      <category term="官方资讯" scheme="http://extheor.club/volantis/categories/%E5%AE%98%E6%96%B9%E8%B5%84%E8%AE%AF/"/>
    
    
  </entry>
  
  <entry>
    <title>心率测量工具限免活动</title>
    <link href="http://extheor.club/volantis/news/2020-02-18/"/>
    <id>http://extheor.club/volantis/news/2020-02-18/</id>
    <published>2020-02-17T16:00:00.000Z</published>
    <updated>2020-11-13T08:51:58.864Z</updated>
    
    <content type="html"><![CDATA[<div class="gallery ">              <p><img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/proj/heartmate/screenshot01.jpg'><br><img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/proj/heartmate/screenshot02.jpg'><br><img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/proj/heartmate/screenshot03.jpg'><br><img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/proj/heartmate/screenshot04.jpg'><br><img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/proj/heartmate/screenshot05.jpg'><br><img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/proj/heartmate/screenshot06.jpg'></p>            </div><p>心率管家 App（仅iOS端），专业版不定期限免，欢迎下载体验。</p><p>如果您看到的时候已经过了限免期，可以先下载免费版使用。为了吸引 app 推荐类网站的爬虫进行推荐，专业版的价格通常在0元到68元之间浮动变化的。</p><div class="btns circle center grid5">            <a href='https://apps.apple.com/cn/app/heart-mate-pro-hrm-utility/id1463348922?ls=1'>  <i class='fab fa-apple'></i>  <b>心率管家</b>  <p class='p red'>专业版</p>  <img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/heartmate_pro.png'></a><a href='https://apps.apple.com/cn/app/heart-mate-lite-hrm-utility/id1475747930?ls=1'>  <i class='fab fa-apple'></i>  <b>心率管家</b>  <p class='p green'>免费版</p>  <img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/heartmate_lite.png'></a>          </div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;div class=&quot;gallery &quot;&gt;
              &lt;p&gt;&lt;img src=&#39;https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/proj/heartmate/screenshot01.jpg&#39;&gt;&lt;br&gt;&lt;img sr
      
    
    </summary>
    
    
      <category term="官方资讯" scheme="http://extheor.club/volantis/categories/%E5%AE%98%E6%96%B9%E8%B5%84%E8%AE%AF/"/>
    
    
  </entry>
  
</feed>
